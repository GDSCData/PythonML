{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e7ef92",
   "metadata": {},
   "source": [
    "# 정규식 기본\n",
    "1. p = re.compile(\"원하는 형태\")\n",
    "2. m = p.match(\"비교할 문자열\") : 주어진 문자열의 처음부터 일치하는 지 확인\n",
    "3. m = p.search(\"비교할 문자열\") : 주어진 문자열 중에 일치하는게 있는 지 확인\n",
    "4. lst = p.findall(\"비교할 문자열\") : 일치하는 모든 것을 \"리스트\" 형태로 반환\n",
    "\n",
    "* 원하는 형태 : 정규식\n",
    "    - . (ca.e) : 하나의 문자를 의미 > care, cafe, case (O) | caffe(X)\n",
    "    - ^ (^de) : 문자열의 시작 > desk, destination (O) | fade(X)\n",
    "    - $ (se$) : 문자열의 끝 > case, base (O) | face(X)\n",
    "    \n",
    "* 공부할 때 참고할 사이트\n",
    "    - https://www.w3schools.com/python/python_regex.asp\n",
    "    - https://docs.python.org/ko/3/library/re.html (공식 문서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f9503d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.group():  care\n",
      "m.string:  careless\n",
      "m.start():  0\n",
      "m.end():  4\n",
      "m.span():  (0, 4)\n",
      "['care', 'cafe']\n"
     ]
    }
   ],
   "source": [
    "import re # 정규식 라이브러리\n",
    "\n",
    "# abcd, book, desk\n",
    "# ca?e\n",
    "# care, cafe, case, cave\n",
    "p = re.compile(\"ca.e\") \n",
    "# . (ca.e) : 하나의 문자를 의미 > care, cafe, case | caffe => x\n",
    "# ^ (^de) : 문자열의 시작 > desk, destination | fade => x\n",
    "# $ (se$) : 문자열의 끝 > case, base (O) | face => x\n",
    "\n",
    "def print_match(m):\n",
    "    if m:\n",
    "        print(\"m.group(): \", m.group()) # 일치하는 문자열 반환\n",
    "        print(\"m.string: \", m.string) # 입력받은 문자열\n",
    "        print(\"m.start(): \", m.start()) # 일치하는 문자열의 시작 index\n",
    "        print(\"m.end(): \", m.end()) # 일치하는 문자열의 끝 index\n",
    "        print(\"m.span(): \", m.span()) # 일치하는 문자열의 시작 / 끝 index\n",
    "    else:\n",
    "        print(\"매칭되지 않았음\")\n",
    "        \n",
    "# match : 주어진 문자열의 처음부터 일치하는지 확인\n",
    "m = p.match(\"case\")\n",
    "n = p.match(\"caffe\")\n",
    "a = p.match(\"careless\") # 주어진 문자열의 처음부터 일치하는지 확인\n",
    "# print(m.group()) # 매치되지 않으면 에러가 발생\n",
    "# print(n.group()) # AttributeError: 'NoneType' object has no attribute 'group'\n",
    "print_match(a)\n",
    "\n",
    "# search : 주어진 문자열 중에 일치하는게 있는지 확인\n",
    "m = p.search(\"good care\")\n",
    "m = p.search(\"careless\")\n",
    "# print_match(m)\n",
    "\n",
    "# findall : 일치하는 모든 것을 리스트 형태로 반환\n",
    "lst = p.findall(\"good_care cafe\")\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ddead",
   "metadata": {},
   "source": [
    "# User Agent\n",
    "- 브라우저가 웹 페이지를 요청할 때 전달하는 헤더의 내용을 바탕으로 서버에서는 어떤 페이지를 보여줄 지를 결정\n",
    "- ex) 스마트폰용 모바일 페이지, 일반 데스크탑용 일반 페이지\n",
    "- 일반적인 정보가 아닐 때는 서버에서 접속을 막거나 권한을 주지 않는 경우가 있다.\n",
    "    - user agent를 변경을 해야함.\n",
    "\n",
    "* 확인 사이트: https://www.whatsmyua.info/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db7ce344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://nadocoding.tistory.com\"\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "res = requests.get(url, headers=headers)\n",
    "res.raise_for_status()\n",
    "\n",
    "with open(\"nadocoding.html\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f0f47",
   "metadata": {},
   "source": [
    "# 웹 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a814fc",
   "metadata": {},
   "source": [
    "* requests : 웹페이지 가져오기 라이브러리\n",
    "* bs4(BeautifulSoup) : 웹페이지 분석(크롤링) 라이브러리\n",
    "* 파싱(Parsing)이란? : 문자열 의미를 분석하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 웹페이지 가져오기\n",
    "res = requests.get('http://~~')\n",
    "\n",
    "# 웹페이지 파싱하기\n",
    "soup = BeautifulSoup(res.content, 'html.parser') # soup에 HTML파일을 파싱한 정보가 들어간다.\n",
    "# 필요 데이터 추출하기\n",
    "data = soup.find('title')\n",
    "\n",
    "# 추출 데이터 활용하기\n",
    "print(data.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9aad1",
   "metadata": {},
   "source": [
    "## 파싱 정보에서 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c3353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">1. 안녕하세요</h1>\n",
      "1. 안녕하세요\n",
      "1. 안녕하세요\n",
      "2. 하이헬로우\n",
      "3. 굳이에요!!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1 id='title'>1. 안녕하세요</h1>\n",
    "        <h2 class='classh2'>2. 하이헬로우</h2>\n",
    "        <h2 id='body' align='center'>3. 굳이에요!!</h2>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "data = soup.find('h1')\n",
    "print(data)\n",
    "print(data.string)\n",
    "print(data.get_text())\n",
    "\n",
    "# h2 태그 문장이 2개 이상인 경우 특정 하나만 가져오기\n",
    "data = soup.find('h2', class_='classh2')\n",
    "data = soup.find('h2', 'classh2')\n",
    "data = soup.find('h2', attrs={'align':'center'})\n",
    "data = soup.find(id='body')\n",
    "\n",
    "# h2 태그 문장 모두를 가져오기\n",
    "datas = soup.find_all('h2')\n",
    "for data in datas:\n",
    "    print(data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "823ef739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 강사가 실제 사용하는 자동 프로그램 소개\n",
      "2. 필요한 프로그램 설치 시연\n",
      "3. 데이터를 엑셀 파일로 만들기\n",
      "4. 엑셀 파일 이쁘게! 이쁘게!\n",
      "5. 나대신 주기적으로 파이썬 프로그램 실행하기\n",
      "6. 파이썬으로 슬랙(slack) 메신저에 글쓰기\n",
      "7. 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기\n",
      "8. 네이버 API 사용해서, 블로그에 글쓰기\n",
      "9. 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "# find()로 더 크게 감싸는 HTML 태그로 추출하고\n",
    "# 다시 추출된 데이터에서 find_all()로 원하는 부분을 추출\n",
    "# strip() 함수 / split() 함수 사용\n",
    "hobby_list = soup.find('ul', id='dev_course_list')\n",
    "items = hobby_list.find_all('li', 'course')\n",
    "for index, item in enumerate(items):\n",
    "    title = item.get_text().split('-')[-1].split('[')[0].strip()\n",
    "    print ('%d. %s' %(index+1, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6188326",
   "metadata": {},
   "source": [
    "## CSS Selector\n",
    "- select() 안에 태그 또는 CSS class 이름등을 넣어줌.\n",
    "- 결과값은 '리스트'형태로 반환된다.\n",
    "    - 매칭되는 첫번째 데이터만 얻고자 할 때: select_one() => 해당 아이템 객체가 리턴\n",
    "   \n",
    "- select('li') : html 파싱 데이터에서 모든 li 태그의 데이터를 가져옴\n",
    "- select('html body h2') : html 하위 body 하위의 h2 태그의 데이터를 모두 가져와 리스트 객체로 리턴\n",
    "- select('ul > li') : ul 태그 바로 아래의 태그 데이터를 가져온다.\n",
    "    - '>' 는 바로 아래 태그인 경우를 선택\n",
    "- select('.h2class') : h2class 인 클래스 이름을 가진 태그 데이터들을 모두 가져온다.\n",
    "    - '.클래스 이름' 으로 검색\n",
    "- select('#h2id') : h2id 인 id 이름을 가진 태그 데이터를 가져온다.\n",
    "    - '#id 이름' 으로 검색\n",
    "- select('li.liclass1.liclass2') : li 태그 중 liclass1, liclass2 클래스 이름을 가진 태그 데이터를 가져온다.\n",
    "    - '태그.클래스이름1.클래스이름2. ...' : class 이름이 여러 개의 경우 해당 class 이름을 모두 가진 태그 데이터를 가져옴.\n",
    "- select('ul#ul_list li.liclass1') : ul 태그 중 id 값이 ul_list 하위에 li 태그 중 클래스 이름이 liclass1 데이터를 가져옴\n",
    "- select_one('ul#ul_list > li.liclass1.liclass2'): ul 태그 중 id 값이 ul_list 바로 아래 태그 중에서 liclass1, liclass2 클래스 이름을 가진 li 태그 데이터 값을 하나 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab7c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(왕초보) - 클래스 소개\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "(왕초보) - 초간단 페이지 만들어보기\n",
      "(왕초보) - 이쁘게 테마 적용해보기\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n",
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test_css.html')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "items = soup.select('li')\n",
    "for item in items:\n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c9faed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "# items = soup.select('ul#hobby_course_list > li')\n",
    "items = soup.select('ul#dev_course_list > li.course.paid')\n",
    "for index, item in enumerate(items):\n",
    "    print('%d. %s' % (index+1, item.get_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9eb10",
   "metadata": {},
   "source": [
    "## 엑셀에 저장하기\n",
    "- openpyxl 라이브러리 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3348706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import os.path\n",
    "\n",
    "def write_excel_template(filename, sheetname, listdata):\n",
    "    if os.path.isfile(filename): # 엑셀 파일이 존재하는 경우\n",
    "        excel_file = openpyxl.load_workbook(filename)\n",
    "        excel_sheet = excel_file.active\n",
    "        excel_sheet = excel_file.create_sheet(title = sheetname)\n",
    "    else: # 엑셀 파일이 존재하지 않는 경우\n",
    "        excel_file = openpyxl.Workbook()\n",
    "        excel_sheet = excel_file.active\n",
    "        if sheetname != '':\n",
    "            excel_sheet.title = sheetname\n",
    "    \n",
    "    excel_sheet.column_dimensions['A'].width = 100\n",
    "    excel_sheet.column_dimensions['B'].width = 20\n",
    "    \n",
    "    for item in listdata:\n",
    "        excel_sheet.append(item)\n",
    "        \n",
    "    excel_file.save(filename)\n",
    "    excel_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8348e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://davelee-fun.github.io/'\n",
    "product_list = list()\n",
    "\n",
    "for page in range(1, 9):\n",
    "    if page == 1:\n",
    "        res = requests.get(URL)\n",
    "    else:\n",
    "        res = requests.get(URL+'page%d/' % (page))\n",
    "    soup = BeautifulSoup(res.content, 'html.parser') # 웹 페이지 데이터 파싱\n",
    "    \n",
    "    product_infos = soup.select('div.listrecent div.card-group div.card')\n",
    "    for product_info in product_infos:\n",
    "        product_name = product_info.select_one('div.card-body > h4.card-text').get_text().strip()\n",
    "        product_date = product_info.select_one('div.card-footer div.wrapfooter span.author-meta span.post-date').get_text().strip()\n",
    "        \n",
    "        product_list.append([product_name, product_date])\n",
    "    # print(res.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c199d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_excel_template('tmp.xlsx', '상품정보2', product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b5d2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 읽기\n",
    "excel_file = openpyxl.load_workbook('tmp.xlsx')\n",
    "excel_sheet = excel_file['상품정보']\n",
    "\n",
    "for item in excel_sheet.rows:\n",
    "    print(item[0].value, item[1].value)\n",
    "excel_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7df843",
   "metadata": {},
   "source": [
    "## Indeed 서울 지역 / 검색어에 따른 일자리 추출(100개씩만)\n",
    "## 그 후 csv 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f34233a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_job_infos_write_csv(search_word):\n",
    "    URL = f\"https://kr.indeed.com/%EC%B7%A8%EC%97%85?q={search_word}&l=%EC%84%9C%EC%9A%B8&start=\"\n",
    "    \n",
    "    filename = \"job-infos.csv\"\n",
    "    f = open(filename, \"w\", encoding=\"utf-8-sig\", newline = \"\")\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(['직종', '회사명'])\n",
    "    \n",
    "    for i in range(0, 110, 10):\n",
    "        R_URL = URL + str(i)\n",
    "        res = requests.get(R_URL)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        jobs = soup.select('div#mosaic-zone-jobcards div#mosaic-provider-jobcards a.tapItem div.slider_container div.slider_list div.slider_item div.job_seen_beacon table.jobCard_mainContent')\n",
    "        \n",
    "        for index, job in enumerate(jobs):\n",
    "            job_name = job.select_one('h2.jobTitle > span')['title'].strip()\n",
    "            job_company = job.select_one('span.companyName').get_text()\n",
    "            writer.writerow([job_name, job_company])\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24bb6381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "찾고자 하는 직종을 입력해주세요: django\n"
     ]
    }
   ],
   "source": [
    "search_word = input(\"찾고자 하는 직종을 입력해주세요: \")\n",
    "extract_job_infos_write_csv(search_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec065160",
   "metadata": {},
   "source": [
    "# CSV 기본 (네이버 금융)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77471bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['151', '후성', '21,250', '250', '-1.16%', '500', '19,679', '92,607', '7.93', '1,161,093', '71.55', '2.79', '']\n",
      "['152', 'KODEX MSCI Korea TR', '12,860', '70', '-0.54%', '0', '19,573', '152,200', '3.73', '295,889', 'N/A', 'N/A', '']\n",
      "['153', '농심', '318,500', '3,000', '+0.95%', '5,000', '19,373', '6,083', '12.55', '30,199', '18.15', '7.50', '']\n",
      "['154', '영원무역', '43,350', '50', '+0.12%', '500', '19,209', '44,311', '27.91', '44,771', '8.33', '8.03', '']\n",
      "['155', '두산', '116,000', '1,500', '-1.28%', '5,000', '19,168', '16,524', '10.58', '57,865', '-66.82', '-34.34', '']\n",
      "['156', 'TIGER 미국나스닥100', '86,835', '245', '+0.28%', '0', '18,574', '21,390', '0.01', '106,609', 'N/A', 'N/A', '']\n",
      "['157', '대웅', '31,600', '300', '+0.96%', '500', '18,373', '58,142', '4.36', '61,421', '17.40', '15.10', '']\n",
      "['158', '한미반도체', '36,600', '500', '+1.39%', '200', '18,102', '49,460', '6.40', '544,525', '21.76', '21.03', '']\n",
      "['159', 'KODEX 레버리지', '23,905', '275', '-1.14%', '0', '17,642', '73,800', '1.29', '9,610,218', 'N/A', 'N/A', '']\n",
      "['160', '현대백화점', '74,700', '100', '+0.13%', '5,000', '17,482', '23,402', '18.15', '27,966', '11.30', '1.65', '']\n",
      "['161', 'LS', '54,200', '600', '+1.12%', '5,000', '17,452', '32,200', '13.97', '75,253', '6.81', '3.94', '']\n",
      "['162', '한전KPS', '38,150', '100', '-0.26%', '200', '17,168', '45,000', '6.41', '236,281', '14.21', '8.23', '']\n",
      "['163', '오뚜기', '466,500', '1,500', '-0.32%', '5,000', '17,130', '3,672', '11.78', '2,062', '15.49', '7.93', '']\n",
      "['164', '대웅제약', '146,500', '500', '+0.34%', '2,500', '16,974', '11,587', '6.84', '37,364', '147.09', '2.38', '']\n",
      "['165', 'JB금융지주', '8,590', '80', '-0.92%', '5,000', '16,921', '196,983', '41.76', '525,957', '3.54', '10.06', '']\n",
      "['166', '현대엘리베이', '41,400', '0', '0.00%', '5,000', '16,897', '40,815', '28.31', '85,036', '26.68', '9.78', '']\n",
      "['167', '신풍제약', '31,750', '550', '+1.76%', '500', '16,823', '52,985', '10.97', '542,757', '5,291.67', '1.72', '']\n",
      "['168', 'LS ELECTRIC', '56,000', '0', '0.00%', '5,000', '16,800', '30,000', '18.00', '72,371', '20.73', '6.15', '']\n",
      "['169', 'KODEX 단기채권', '103,095', '0', '0.00%', '0', '16,670', '16,170', '0.00', '10,698', 'N/A', 'N/A', '']\n",
      "['170', 'DGB금융지주', '9,770', '30', '-0.31%', '5,000', '16,526', '169,146', '48.81', '641,303', '3.49', '6.93', '']\n",
      "['171', 'NHN', '84,000', '9,200', '-9.87%', '500', '16,435', '19,565', '18.31', '434,673', '50.60', '1.35', '']\n",
      "['172', '케이카', '33,000', '750', '+2.33%', '500', '15,869', '48,087', '18.23', '410,714', '54.82', '15.11', '']\n",
      "['173', '명신산업', '29,900', '700', '-2.29%', '500', '15,689', '52,470', '3.17', '663,746', '-12.97', '-67.39', '']\n",
      "['174', 'TIGER MSCI Korea TR', '16,140', '85', '-0.52%', '0', '15,543', '96,300', '0.00', '2,164', 'N/A', 'N/A', '']\n",
      "['175', 'HDC현대산업개발', '23,500', '0', '0.00%', '5,000', '15,488', '65,907', '10.37', '120,031', '5.11', '8.85', '']\n",
      "['176', 'PI첨단소재', '52,400', '800', '-1.50%', '500', '15,388', '29,366', '13.51', '101,288', '26.31', '15.62', '']\n",
      "['177', '동국제강', '16,050', '0', '0.00%', '5,000', '15,317', '95,433', '23.15', '376,620', '3.39', '3.35', '']\n",
      "['178', '에스엘', '31,550', '400', '-1.25%', '500', '15,208', '48,203', '15.69', '136,704', '10.46', '4.74', '']\n",
      "['179', '한국앤컴퍼니', '16,000', '150', '-0.93%', '500', '15,190', '94,935', '9.72', '57,077', '6.14', '5.08', '']\n",
      "['180', '동원시스템즈', '51,700', '700', '-1.34%', '5,000', '15,090', '29,188', '13.22', '22,719', '23.41', '10.97', '']\n",
      "['181', '대한전선', '1,745', '5', '-0.29%', '500', '14,945', '856,473', '2.56', '5,051,685', '44.74', '0.92', '']\n",
      "['182', 'KODEX 삼성그룹', '9,880', '30', '-0.30%', '0', '14,909', '150,900', '1.46', '63,722', 'N/A', 'N/A', '']\n",
      "['183', 'KODEX 종합채권(AA-이상)액티브', '108,400', '0', '0.00%', '0', '14,835', '13,685', '0.00', '5,411', 'N/A', 'N/A', '']\n",
      "['184', '아시아나항공', '19,650', '500', '-2.48%', '5,000', '14,622', '74,412', '7.85', '756,848', '-3.38', '-43.83', '']\n",
      "['185', 'LIG넥스원', '66,100', '3,400', '+5.42%', '5,000', '14,542', '22,000', '6.66', '550,197', '15.41', '8.86', '']\n",
      "['186', '한일시멘트', '20,600', '400', '-1.90%', '500', '14,268', '69,262', '2.51', '164,318', '13.57', 'N/A', '']\n",
      "['187', 'TIGER 단기통안채', '100,500', '0', '0.00%', '0', '14,254', '14,183', '0.86', '62,397', 'N/A', 'N/A', '']\n",
      "['188', '아이에스동서', '44,500', '800', '+1.83%', '500', '13,747', '30,893', '5.48', '152,284', '25.49', '10.92', '']\n",
      "['189', '롯데리츠', '5,650', '10', '+0.18%', '500', '13,728', '242,969', '9.61', '291,469', '35.31', '1.68', '']\n",
      "['190', '종근당', '120,000', '4,000', '+3.45%', '2,500', '13,711', '11,426', '7.96', '175,444', '26.21', '17.88', '']\n",
      "['191', '롯데렌탈', '37,100', '50', '-0.13%', '5,000', '13,591', '36,634', '1.50', '59,498', '12.86', '6.73', '']\n",
      "['192', '금호타이어', '4,620', '55', '+1.20%', '5,000', '13,271', '287,260', '4.32', '455,548', '-18.48', '-6.74', '']\n",
      "['193', 'DL', '63,000', '900', '-1.41%', '5,000', '13,202', '20,956', '12.40', '54,699', '2.00', '11.84', '']\n",
      "['194', 'F&F홀딩스', '33,650', '300', '+0.90%', '500', '13,162', '39,114', '0.86', '54,648', '0.28', '16.59', '']\n",
      "['195', '한화투자증권', '6,090', '240', '-3.79%', '5,000', '13,066', '214,548', '6.35', '4,804,254', '10.24', '5.58', '']\n",
      "['196', 'LG생활건강우', '620,000', '4,000', '+0.65%', '5,000', '13,018', '2,100', '84.21', '686', '12.90', 'N/A', '']\n",
      "['197', '코스모신소재', '43,100', '600', '-1.37%', '1,000', '12,967', '30,087', '6.59', '305,462', '55.26', '7.16', '']\n",
      "['198', '녹십자홀딩스', '27,500', '1,250', '+4.76%', '500', '12,933', '47,028', '5.21', '145,984', '8.66', '19.74', '']\n",
      "['199', '롯데칠성', '136,000', '1,000', '-0.73%', '500', '12,619', '9,279', '8.87', '7,532', '16.07', '-1.00', '']\n",
      "['200', '대한유화', '193,500', '500', '-0.26%', '5,000', '12,578', '6,500', '12.30', '16,402', '6.02', '6.93', '']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://finance.naver.com/sise/sise_market_sum.nhn?sosok=0&page=\"\n",
    "\n",
    "filename = \"시가총액1-200.csv\"\n",
    "f = open(filename, \"w\", encoding=\"utf-8-sig\", newline = \"\")\n",
    "# newline = \"\" => 이렇게 해주지 않으면 각 리스트 내용 사이에 띄어쓰기가 존재함.\n",
    "# 해당 파일이 excel에서 열 때 한글이 깨지면 encoding 방식을 \"utf8\" 대신 \"utf-8-sig\"형식으로 지정\n",
    "writer = csv.writer(f)\n",
    "\n",
    "title = \"N\t종목명\t현재가\t전일비\t등락률\t액면가\t시가총액\t상장주식수\t외국인비율\t거래량\tPER\tROE\t토론실\".split('\\t')\n",
    "writer.writerow(title)\n",
    "\n",
    "for page in range(1, 5):\n",
    "    res = requests.get(url + str(page))\n",
    "    res.raise_for_status() # 접속에 문제가 있는지 확인\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    \n",
    "    # 의미 없는 데이터를 가져오지 않기 위한 내 방법\n",
    "    # data_rows = soup.find(\"table\", attrs={\"class\":\"type_2\"}).find(\"tbody\").find_all(\"tr\", attrs={\"onmouseout\":\"mouseOut(this)\"})\n",
    "    data_rows = soup.find(\"table\", attrs={\"class\":\"type_2\"}).find(\"tbody\").find_all(\"tr\")\n",
    "    for row in data_rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) <= 1: # 의미 없는 데이터는 skip\n",
    "            continue\n",
    "        data = [column.get_text().strip() for column in columns]\n",
    "        # print(data)\n",
    "        if page == 4:\n",
    "            print(data)\n",
    "        writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
